{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from goose3 import Goose\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall(folder_path):\n",
    "\n",
    "    # Loading the BERT T5 model and the sentiment analysis model\n",
    "    summarize_tokenizer, summarizer_model = load_summarizing_model()\n",
    "    # sentiment_tokenizer, sentiment_model = load_sentiment_analysis_model()\n",
    "\n",
    "    json_files = get_json_files(folder_path)\n",
    "    json_files.reverse()\n",
    "    #json_files = json_files[2:]\n",
    "    print(f\"Found {len(json_files)} JSON files \")\n",
    "    for file in json_files:\n",
    "        #Trying reading the file\n",
    "        try:\n",
    "            df = pd.read_json(folder_path+\"/\"+ file)\n",
    "            link = df['Link']\n",
    "        except KeyError:\n",
    "            print(\"The column 'Link' does not exist in the DataFrame.\")\n",
    "            continue\n",
    "\n",
    "        for url in link:\n",
    "            # Scrape the url\n",
    "            long_text = scraping_web_page(url)\n",
    "\n",
    "            # Summarize the text\n",
    "            summary = long_text if len(long_text) < 470 else summarize_text(summarize_tokenizer, summarizer_model, long_text)\n",
    "\n",
    "            # Compute the sentiment score\n",
    "            score = computing_sentiment_score(sentiment_tokenizer, sentiment_model, summary)\n",
    "\n",
    "            if score is None:\n",
    "                score=0\n",
    "\n",
    "            # Assign summary and sentiment score to respective rows\n",
    "            df.loc[df['Link'] == url, \"summary\"] = summary\n",
    "            df.loc[df['Link'] == url, \"sentiment_score\"] = score\n",
    "\n",
    "        # Save the processed DataFrame to a JSON file\n",
    "        df.to_json('/content/drive/MyDrive/Article_project/processed_with_summary/' + \"summary_\"+file, orient='records')\n",
    "        move_file_to_nested_folder(folder_path+\"/\"+ file)\n",
    "        print('')\n",
    "        print('µµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµ***')\n",
    "        print(f\"****Finished processing {file}*****\")\n",
    "        print('µµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµ***')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computing_sentiment_score(tokenizer, model, text):\n",
    "    try:\n",
    "        tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "        result = model(tokens)\n",
    "        score = int(torch.argmax(result.logits))+1\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print('a problem occurs when computing the sentiment')\n",
    "        return None\n",
    "    except RuntimeError as e:\n",
    "        print('A problem occurs when computing the sentiment:', e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentiment_analysis_model():\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "        model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print('a problem occurs when loading the sentiment analysis model')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(folder_path):\n",
    "    json_files = get_json_files(folder_path)\n",
    "    print(f\"Found {len(json_files)} JSON files \")\n",
    "\n",
    "    my_list = [\n",
    "        \"An Errorrr happen wheb trying to reach the website\",\n",
    "        \"Access denied Errorrr\",\n",
    "        \"Network Errorrr. code 400\",\n",
    "        \"Extraction Errorrr\",\n",
    "        \"\"\n",
    "    ]\n",
    "\n",
    "    for file in json_files:\n",
    "        # Trying to read the file\n",
    "        try:\n",
    "            df = pd.read_json(os.path.join(folder_path, file))\n",
    "            \n",
    "            # Define a function to apply the labeling logic\n",
    "            def label_row(row):\n",
    "                if row['summary'] in my_list:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1\n",
    "\n",
    "            # Apply the function to each row and create a new column 'label'\n",
    "            df['label'] = df.apply(label_row, axis=1)\n",
    "\n",
    "             # Save the DataFrame back to the JSON file\n",
    "            #df.to_json(os.path.join('processed_with_label', file), orient='records', lines=True)\n",
    "\n",
    "            # Save the processed DataFrame to a JSON file\n",
    "            df.to_json('processed_with_label/' + \"summary_\"+file, orient='records')\n",
    "\n",
    "            print('')\n",
    "            print('µµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµ***')\n",
    "            print(f\"****Finished processing {file}*****\")\n",
    "            print('µµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµ***')\n",
    "            print('')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error happened: {e}\")\n",
    "            print(f\"****ERRROR processing {file}*****\")\n",
    "            print('')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_recurrent_label(folder_path):\n",
    "    json_files = get_json_files(folder_path)\n",
    "    print(f\"Found {len(json_files)} JSON files \")\n",
    "\n",
    "    for file in json_files:\n",
    "        # Trying to read the file\n",
    "        \n",
    "            df = pd.read_json(os.path.join(folder_path, file))\n",
    "\n",
    "            df_filtered = df[df['label'] == 1]\n",
    "\n",
    "            if df_filtered.empty:\n",
    "                continue\n",
    "\n",
    "            sentiment_score_counts = df_filtered['sentiment_score'].value_counts()\n",
    "\n",
    "            most_common_score_sentiment = sentiment_score_counts.idxmax()\n",
    "            least_common_score_sentiment = sentiment_score_counts.idxmin()\n",
    "\n",
    "            df['most_recurrent_sentiment_score'] = most_common_score_sentiment\n",
    "            df['least_recurrent_sentiment_score'] = least_common_score_sentiment\n",
    "\n",
    "            # Save the processed DataFrame to a JSON file\n",
    "            df.to_json('processed_with_min_max/' + \"summary_\"+file, orient='records')\n",
    "\n",
    "            print('')\n",
    "            print(f\"****Finished processing {file}*****\")\n",
    "            print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_recurrent_summaries(folder_path, type_column=\"sentiment_score\"):\n",
    "    json_files = get_json_files(folder_path)\n",
    "    #json_files = json_files[5:10]\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for file in json_files:\n",
    "        # Load JSON data into a DataFrame\n",
    "        df = pd.read_json(folder_path + \"/\" + file)\n",
    "        df_filtered = df[df[\"label\"] != 0]\n",
    "\n",
    "        # Check if the DataFrame is empty after filtering\n",
    "        if df_filtered.empty:\n",
    "            print(f\"Skipping {file} as it is empty after filtering.\")\n",
    "            continue\n",
    "\n",
    "        # Extract scores\n",
    "        df_most_recurrent_score_value = df[\"most_recurrent_sentiment_score\"].iloc[0]\n",
    "        df_least_recurrent_score_value = df[\"least_recurrent_sentiment_score\"].iloc[0]\n",
    "\n",
    "        df_most_recurrent_score = df_filtered[df_filtered[\"sentiment_score\"] == df_most_recurrent_score_value]\n",
    "        df_least_recurrent_score = df_filtered[df_filtered[\"sentiment_score\"] == df_least_recurrent_score_value]\n",
    "\n",
    "        df_summary_most_recurrent_score = df_most_recurrent_score[\"summary\"]\n",
    "        df_summary_least_recurrent_score = df_least_recurrent_score[\"summary\"]\n",
    "\n",
    "        print(len(df_summary_least_recurrent_score))\n",
    "        print(len(df_summary_most_recurrent_score))\n",
    "\n",
    "        # Initialize summaries\n",
    "        most_recurrent_summaries = [\"\" for _ in range(5)]\n",
    "        least_recurrent_summaries = [\"\" for _ in range(5)]\n",
    "\n",
    "        # Fill summaries\n",
    "        for i in range(min(5, len(df_summary_most_recurrent_score))):\n",
    "            most_recurrent_summaries[i] = \".\".join(df_summary_most_recurrent_score.iloc[:i+1])\n",
    "\n",
    "        for i in range(min(5, len(df_summary_least_recurrent_score))):\n",
    "            least_recurrent_summaries[i] = \".\".join(df_summary_least_recurrent_score.iloc[:i+1])\n",
    "\n",
    "        # Add new columns with the most and least recurrent types\n",
    "        for i in range(5):\n",
    "            df[f'most_recurrent_summary_{i+1}'] = most_recurrent_summaries[i]\n",
    "            df[f'least_recurrent_summary_{i+1}'] = least_recurrent_summaries[i]\n",
    "\n",
    "        # Save the modified DataFrame back to JSON (or any other format)\n",
    "        #df.to_json(f'processed_with_recurrent_summary/recurrent_summary_{file}', orient='records')\n",
    "        df.to_json(f'processed_with_recurrent_summaries/{file}', orient='records')\n",
    "        \n",
    "        print('')\n",
    "        print('µµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµ***')\n",
    "        print(f\"****Finished processing {file}*****\")\n",
    "        print('µµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµµ***')\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(folder_path, prefix_to_remove):\n",
    "    # List all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    for file in files:\n",
    "        # Check if the file name starts with the prefix to remove\n",
    "        if file.startswith(prefix_to_remove):\n",
    "            # Create the new file name by removing the prefix\n",
    "            new_file_name = file[len(prefix_to_remove):]\n",
    "            \n",
    "            # Create the full path for the old and new file names\n",
    "            old_file_path = os.path.join(folder_path, file)\n",
    "            new_file_path = os.path.join(folder_path, new_file_name)\n",
    "            \n",
    "            # Rename the file\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            print(f\"Renamed '{file}' to '{new_file_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_web_page(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except:\n",
    "            print(\"An Errorrr happen wheb trying to reach the website\")\n",
    "            return (\"An Errorrr happen wheb trying to reach the website\")\n",
    "\n",
    "        # Check if access is denied (e.g., HTTP status code 403 or 401)\n",
    "        if response.status_code == 403 or response.status_code == 401:\n",
    "            print(\"Access denied. Website does not allow scraping.\")\n",
    "            return \"Access denied Errorrr\"\n",
    "        elif response.status_code == 400:\n",
    "            print(\"Network Errorrr. code 400\")\n",
    "            return (\"Network Errorrr. code 400\")\n",
    "        elif response.status_code == 404:\n",
    "            print(\"Network Errorrr. code 400\")\n",
    "            return (\"Network Errorrr. code 400\")\n",
    "\n",
    "        config = {'strict': True, 'http_timeout': 360.0}\n",
    "        g = Goose(config)\n",
    "        article = g.extract(url=url)\n",
    "        print(\"******success *****\")\n",
    "        return article.cleaned_text\n",
    "    except Exception as e:\n",
    "        print(\"An exception occurs:\", e)\n",
    "        return \"Extraction Errorrr\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_summarizing_model():\n",
    "    '''\n",
    "    This function load the summarizer then return the tokenizer and the model\n",
    "    '''\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained('t5-base', return_dict=True)\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print('A problem happen when loading the summarizing model')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computing_sentiment_score(tokenizer, model, text):\n",
    "    try:\n",
    "        tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "        result = model(tokens)\n",
    "        score = int(torch.argmax(result.logits))+1\n",
    "        return score\n",
    "    except exception as e:\n",
    "        print('a problem occurs when computing the sentiment')\n",
    "        return None\n",
    "    except RuntimeError as e:\n",
    "        print('A problem occurs when computing the sentiment:', e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove special characters (except \".\") and extra whitespaces\n",
    "    cleaned_text = re.sub(r'[^\\w\\s.]', '', text)\n",
    "    # Replace multiple consecutive whitespaces with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(tokenizer, model, long_text):\n",
    "    sequence = (long_text)\n",
    "    inputs=tokenizer.encode(\"sumarize: \" +sequence,return_tensors='pt', max_length=512, truncation=True)\n",
    "    output = model.generate(inputs, min_length=300, max_length=512)\n",
    "\n",
    "    # Decode the output without special tokens\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Clean up the summary\n",
    "    cleaned_summary = clean_text(summary)\n",
    "    return cleaned_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_files(folder):\n",
    "    \"\"\"\n",
    "    Returns a list of all files ending with \".csv\" in the current directory\n",
    "    \"\"\"\n",
    "    files = os.listdir(folder) # Get a list of all files in the current directory\n",
    "    json_files = [file for file in files if file.endswith(\".json\")] # filter to keep json file only\n",
    "    return json_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def move_file_to_nested_folder(file_path, target_dir=\"processed_json_file\"):\n",
    "    \"\"\"\n",
    "    Move a file to a nested folder within the current directory\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The full path to the fil to be moved\n",
    "        target_dir (str, optional): The name of the nested folder. Default to \"processed_json_file\"\n",
    "\n",
    "    \"\"\"\n",
    "    nested_folder_path = os.path.join(os.getcwd(), target_dir)\n",
    "    if not os.path.exists(nested_folder_path):\n",
    "        os.makedirs(nested_folder_path)\n",
    "\n",
    "    # Move the file to the nested folder\n",
    "    shutil.move(file_path, nested_folder_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
